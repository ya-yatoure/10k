{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2TokenizerFast, DistilBertModel, DistilBertTokenizerFast\n",
    "from torch import nn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DistilBERT tokenizer\n",
    "distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(\"/Users/benjaminhaussmann/Desktop/Dissertation_Coding/2019_data/text_covars_to512_2019.csv\")\n",
    "\n",
    "dataset_fraction = 0.02  # use frac of data\n",
    "\n",
    "# Sample dataset_fraction of the data\n",
    "df = df.sample(frac=dataset_fraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit initial regression model\n",
    "initial_regressor = LinearRegression()\n",
    "initial_regressor.fit(df[['lev', 'logEMV', 'naics2']], df['ER_1'])\n",
    "\n",
    "# Get residuals\n",
    "df['ER_1_residuals'] = df['ER_1'] - initial_regressor.predict(df[['lev', 'logEMV', 'naics2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by companies when test/train splitting so we don't have companies that appear in test and trainset\n",
    "unique_companies = df['cik'].unique()\n",
    "train_companies, test_companies = train_test_split(unique_companies, test_size=0.2)\n",
    "\n",
    "train_df = df[df['cik'].isin(train_companies)]\n",
    "test_df = df[df['cik'].isin(test_companies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For Train and Test set:\n",
    "# input IDs and attention masks as PyTorch tensors\n",
    "# structured data to a PyTorch tensor\n",
    "# abnormal returns variable as a PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = distilbert_tokenizer(list(train_df['text']), truncation=True, padding=True)\n",
    "train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
    "train_attention_mask = torch.tensor(train_encodings['attention_mask'])\n",
    "train_target = torch.tensor(train_df['ER_1_residuals'].values, dtype=torch.float)\n",
    "\n",
    "test_encodings = distilbert_tokenizer(list(test_df['text']), truncation=True, padding=True)\n",
    "test_input_ids = torch.tensor(test_encodings['input_ids'])\n",
    "test_attention_mask = torch.tensor(test_encodings['attention_mask'])\n",
    "test_target = torch.tensor(test_df['ER_1_residuals'].values, dtype=torch.float)\n",
    "\n",
    "train_data = TensorDataset(train_input_ids, train_attention_mask, train_target)\n",
    "test_data = TensorDataset(test_input_ids, test_attention_mask, test_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffnet(embeddings)\n\u001b[1;32m     24\u001b[0m         \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m model \u001b[39m=\u001b[39m DistilBERTRegressor()\n\u001b[1;32m     29\u001b[0m \u001b[39m# Move model to GPU if available\u001b[39;00m\n\u001b[1;32m     30\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[47], line 10\u001b[0m, in \u001b[0;36mDistilBERTRegressor.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[39msuper\u001b[39m(DistilBERTRegressor, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistilbert \u001b[39m=\u001b[39m DistilBertModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mdistilbert-base-uncased\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(\u001b[39m0.2\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffnet \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistilbert\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdim, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/draftdual/lib/python3.9/site-packages/transformers/modeling_utils.py:2619\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2618\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2619\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2621\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2622\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/draftdual/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:475\u001b[0m, in \u001b[0;36mDistilBertModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[1;32m    474\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m Embeddings(config)  \u001b[39m# Embeddings\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m Transformer(config)  \u001b[39m# Encoder\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_init()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/draftdual/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:326\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    325\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_layers \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mn_layers\n\u001b[0;32m--> 326\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([TransformerBlock(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mn_layers)])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/draftdual/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:326\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    325\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_layers \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mn_layers\n\u001b[0;32m--> 326\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([TransformerBlock(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mn_layers)])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/draftdual/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:272\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mdim \u001b[39m%\u001b[39m config\u001b[39m.\u001b[39mn_heads \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconfig.n_heads \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mn_heads\u001b[39m}\u001b[39;00m\u001b[39m must divide config.dim \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mdim\u001b[39m}\u001b[39;00m\u001b[39m evenly\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention \u001b[39m=\u001b[39m MultiHeadSelfAttention(config)\n\u001b[1;32m    273\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa_layer_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(normalized_shape\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mdim, eps\u001b[39m=\u001b[39m\u001b[39m1e-12\u001b[39m)\n\u001b[1;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn \u001b[39m=\u001b[39m FFN(config)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/draftdual/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:154\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    151\u001b[0m     \u001b[39m# Raise value errors for even multi-head attention nodes\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mself.n_heads: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads\u001b[39m}\u001b[39;00m\u001b[39m must divide self.dim: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim\u001b[39m}\u001b[39;00m\u001b[39m evenly\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_lin \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(in_features\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdim, out_features\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdim)\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_lin \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mdim, out_features\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mdim)\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_lin \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mdim, out_features\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mdim)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/draftdual/lib/python3.9/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/draftdual/lib/python3.9/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/draftdual/lib/python3.9/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class DistilBERTRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBERTRegressor, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.ffnet = nn.Linear(self.distilbert.config.dim, 1)  # ffnet\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # get embeddings from DistilBERT model\n",
    "        embeddings = self.distilbert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        # pool over sequence dimension\n",
    "        embeddings = embeddings.mean(dim=1)\n",
    "\n",
    "        # pass pooled embeddings through the ffnet\n",
    "        output = self.ffnet(embeddings)\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "\n",
    "model = DistilBERTRegressor()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 10  # adjust as needed\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()  # turn on training mode\n",
    "    total_loss = 0\n",
    "    print(\"Starting epoch\", epoch)\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # Get data\n",
    "        input_ids, attention_mask, targets = batch\n",
    "\n",
    "        # Move data to the GPU if available\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print intermediate loss\n",
    "        if i % 100 == 0:  # print every 100 batches\n",
    "            print(\"Batch: {}, Loss: {}\".format(i, loss.item()))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of sample R-squared: -0.006078599557464814\n"
     ]
    }
   ],
   "source": [
    "# switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    for batch in test_loader:\n",
    "        # Get data\n",
    "        input_ids, attention_mask, targets = batch\n",
    "\n",
    "        # Move data to the GPU if available\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Make predictions\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        actuals.extend(targets.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    # Calculate R-squared score\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print('Out of sample R-squared:', r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "draftdual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4653c8b9c5a9221af54e3a828b47b44e3c5bd821e9ef09a07476c49f7406824"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
