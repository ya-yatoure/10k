{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DistilBert Model \n",
    "\n",
    "##### Create new conde environment: conda create -n draftdual python=3.9\n",
    "##### conda activate draftdual\n",
    "##### pip install torch numpy transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertTokenizerFast\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df = pd.read_csv('/Users/benjaminhaussmann/Desktop/Dissertation Coding/2019_10kdata_with_covars')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly sample a subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sub sample of 100 rows\n",
    "df_sample = df.sample(n=640)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize  text data and convert it into input IDs and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize  tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the text data\n",
    "encodings = tokenizer(list(df_sample['text']), truncation=True, padding=True)\n",
    "\n",
    "# Get the input IDs and attention masks as PyTorch tensors\n",
    "input_ids = torch.tensor(encodings['input_ids'])\n",
    "attention_mask = torch.tensor(encodings['attention_mask'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize structured features ('lev', 'logEMV',  'naics2'), get tensore of feature and outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "structured_data = scaler.fit_transform(df_sample[['lev', 'logEMV', 'naics2']])\n",
    "\n",
    "# Convert the structured data to a PyTorch tensor\n",
    "structured_data = torch.tensor(structured_data, dtype=torch.float)\n",
    "\n",
    "# Get the abnormal returns variable as a PyTorch tensor\n",
    "target = torch.tensor(df_sample['ER_1'].values, dtype=torch.float)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split  data into training and validation sets, create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, val_inputs, train_masks, val_masks, train_structured, val_structured, train_target, val_target = train_test_split(\n",
    "    input_ids, attention_mask, structured_data, target, test_size=0.2)\n",
    "\n",
    "# Create TensorDatasets for the training and validation sets\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_structured, train_target)\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_structured, val_target)\n",
    "\n",
    "# Create DataLoaders for the training and validation sets\n",
    "train_dataloader = DataLoader(train_data, batch_size=16)\n",
    "val_dataloader = DataLoader(val_data, batch_size=16)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dual Input Model\n",
    "#### Input 1: Text sequences from 10K filings that go into DistilBert\n",
    "#### Input 2: structured features ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualInputModel(nn.Module):\n",
    "    def __init__(self, num_structured_features, text_embedding_dim):\n",
    "        super(DualInputModel, self).__init__()\n",
    "\n",
    "\n",
    "        #FREEZE DISTILBERT\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        for param in self.distilbert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfrozen # The DistilBERT model for the text data\n",
    "        #self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        # A feed-forward neural network for the structured data\n",
    "        self.ffnn = nn.Sequential(\n",
    "            nn.Linear(num_structured_features, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)\n",
    "        )\n",
    "\n",
    "        combined_dim = text_embedding_dim + 8\n",
    "\n",
    "        # Linear combination layer\n",
    "        self.combined_layer = nn.Linear(combined_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, structured_data):\n",
    "        # Pass the text data through DistilBERT\n",
    "        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeddings = distilbert_output.last_hidden_state.mean(dim=1)  # Average the sequence dimension\n",
    "\n",
    "        # Pass the structured data through the feed-forward neural network\n",
    "        structured_embeddings = self.ffnn(structured_data)\n",
    "\n",
    "        # Concatenate the text embeddings and structured embeddings\n",
    "        combined = torch.cat((text_embeddings, structured_embeddings), dim=1)\n",
    "\n",
    "        # Pass the combined embeddings through the combined layer\n",
    "        output = self.combined_layer(combined)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parametrs in the nnet del are trained,  the DistilBert models parameters are NOT trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss per observation 12.907300114631653\n",
      "Validation loss per observation 12.824466347694397\n",
      "Train loss per observation 12.420589357614517\n",
      "Validation loss per observation 12.495683252811432\n",
      "Train loss per observation 11.912511110305786\n",
      "Validation loss per observation 12.03365296125412\n",
      "Train loss per observation 11.248340904712677\n",
      "Validation loss per observation 11.436458230018616\n",
      "Train loss per observation 10.528736412525177\n",
      "Validation loss per observation 10.856452107429504\n",
      "Train loss per observation 9.9879439920187\n",
      "Validation loss per observation 10.48949283361435\n",
      "Train loss per observation 9.71215009689331\n",
      "Validation loss per observation 10.327430486679077\n",
      "Train loss per observation 9.566304385662079\n",
      "Validation loss per observation 10.247015297412872\n",
      "Train loss per observation 9.45389948785305\n",
      "Validation loss per observation 10.181807696819305\n",
      "Train loss per observation 9.369076520204544\n",
      "Validation loss per observation 10.150985956192017\n",
      "Train loss per observation 9.331942275166512\n",
      "Validation loss per observation 10.122880280017853\n",
      "Train loss per observation 9.272142872214317\n",
      "Validation loss per observation 10.08966839313507\n",
      "Train loss per observation 9.202324077486992\n",
      "Validation loss per observation 10.080980598926544\n",
      "Train loss per observation 9.125805422663689\n",
      "Validation loss per observation 10.057898581027985\n",
      "Train loss per observation 9.115925773978233\n",
      "Validation loss per observation 10.051264882087708\n",
      "Train loss per observation 9.084213316440582\n",
      "Validation loss per observation 10.036233305931091\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DualInputModel(num_structured_features=3, text_embedding_dim=768).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for dataloader, is_training in [(train_dataloader, True), (val_dataloader, False)]:\n",
    "        total_loss = total_samples = 0\n",
    "\n",
    "        model.train(is_training)\n",
    "        for batch in dataloader:\n",
    "            *inputs, targets = (t.to(device) for t in batch)\n",
    "            targets = targets.unsqueeze(1)\n",
    "\n",
    "            with torch.set_grad_enabled(is_training):\n",
    "                outputs = model(*inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_samples += 1\n",
    "\n",
    "        avg_loss = total_loss / total_samples\n",
    "        print(f\"{'Train' if is_training else 'Validation'} loss per observation {avg_loss}\")\n",
    "        (train_losses if is_training else val_losses).append(avg_loss)\n",
    "\n",
    "\n",
    "# Generate predictions after all epochs\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(val_inputs.to(device), val_masks.to(device), val_structured.to(device))\n",
    "pred = pred.cpu().numpy().flatten()\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(val_target.numpy(), pred)\n",
    "print('R-squared: ', r2)\n",
    "\n",
    "plt.plot(train_losses, label='Training Loss per observation')\n",
    "plt.plot(val_losses, label='Validation Loss per observation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss per observation')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "draftdual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4653c8b9c5a9221af54e3a828b47b44e3c5bd821e9ef09a07476c49f7406824"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
